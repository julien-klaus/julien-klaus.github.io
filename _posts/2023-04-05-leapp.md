---
title: 'Learn Probabilistic Programs'
date: 2023-04-05
permalink: /posts/2023/04/leapp
tags:
  - bayesian network
  - probabilistic programs
  - publication
---

A while ago our paper _[A visual analytics workflow for probabilistic modeling](https://www.sciencedirect.com/science/article/pii/S2468502X23000153)_ was published in the Visual Informatics Journal.

As probabilistic models, probabilistic programs are used. These have the advantage that they have a complete inference backend. Assuming our probabilistic model has two random variables `age` and `married`, this backend allows us to make both marginalization queries like `p(age)`, showing the distribution of `age`, whether `married` or not, and conditioning queries `p(age&#124;married=true)`, computing the distribution of `age` for people who are `married`.

But how do you learn probabilistic programs? After a literature search, we realized that they are not really learned, but rather developed by hand. 

However, we wanted to automatically generate them, and so the idea arose to learn probabilistic programs from Bayesian Networks. For this, I used at the R-package `bnlearn`. This package learns a Bayesian Network given a CSV-file, using different scores. After the learning I translated the Bayesian Network into a desired format, that I can read via Python. After reading the Network I could simply transform the Network into a Blog or PyMC program. You can find the code for this translation at [GitHub](https://github.com/julien-klaus/leapp). Since the setup needs R and Python, I also created a [Docker Image](https://hub.docker.com/repository/docker/julienklaus/leapp/).

To use the Docker Image you simply has to write:
`````
docker run -it julienklaus/leapp:latest
`````

This command downloads and starts the image in an interactive way. The image contains the credit data set. 
````
gender,age,debt,married,bankCustomer,education,ethnicity,yearsEmployed,priorDefault,employed,creditScore,driversLicense,citizen,zipCode,income,approved
b,30.83,0.0,u,g,w,v,1.25,t,t,1,f,g,00202,0,+
a,58.67,4.46,u,g,q,h,3.04,t,t,6,f,g,00043,560,+
a,24.50,0.5,u,g,q,h,1.5,t,f,0,f,g,00280,824,+
b,27.83,1.54,u,g,w,v,3.75,t,t,5,t,g,00100,3,+
b,20.17,5.625,u,g,w,v,1.71,t,f,0,f,s,00120,0,+
b,32.08,4.0,u,g,m,v,2.5,t,f,0,t,g,00360,0,+
````
Containing different personal data and the approval of the credit. Using this data set and the docker image you can learn a probabilistic program with:
````
(leapp) root@d8850545d27d:/leapp# python leapp.py -i data/credit.csv
````
Bnlearn needs the data set in a complete numerical style. So the first step is to create a map of non-numerical values to numerical values. This map is returned by the program as well.
````
You have categorical data with characters in you code. I had to transform it into numerical values. Please note the following mapping:
gender: {0: 'a', 1: 'b'}
married: {0: 'l', 1: 'u', 2: 'y'}
bankCustomer: {0: 'g', 1: 'gg', 2: 'p'}
education: {0: 'aa', 1: 'c', 2: 'cc', 3: 'd', 4: 'e', 5: 'ff', 6: 'i', 7: 'j', 8: 'k', 9: 'm', 10: 'q', 11: 'r', 12: 'w', 13: 'x'}
ethnicity: {0: 'bb', 1: 'dd', 2: 'ff', 3: 'h', 4: 'j', 5: 'n', 6: 'o', 7: 'v', 8: 'z'}
priorDefault: {0: 'f', 1: 't'}
employed: {0: 'f', 1: 't'}
driversLicense: {0: 'f', 1: 't'}
citizen: {0: 'g', 1: 'p', 2: 's'}
approved: {0: '+', 1: '-'}
````
After this the program returns a probabilistic program for this data set. By default it returns a Blog program. The start of this program looks like the following:
````
# BLOG CODE
random Integer priorDefault ~ Categorical({0->0.4655,1->0.5345});
random Integer approved ~
         case priorDefault in {
                 0 -> Categorical({0->0.0592,1->0.9408}),
                 1 -> Categorical({0->0.7966,1->0.2034})
};
random Integer employed ~
         case [priorDefault,approved] in {
                 [0,0] -> Categorical({0->0.9444,1->0.0556}),
                 [0,1] -> Categorical({0->0.7797,1->0.2203}),
                 [1,0] -> Categorical({0->0.2734,1->0.7266}),
                 [1,1] -> Categorical({0->0.7042,1->0.2958})
};
random Integer citizen ~
         case employed in {
                 0 -> Categorical({0->0.8579,1->0.0055,2->0.1366}),
                 1 -> Categorical({0->0.9895,1->0.0,2->0.0105})
};
...
````
In the publication we used PyMC code (`-p` flag). Those models can be used for inference queries, as described above. To do so interactively we probosed Lumen. A tool for interactively exploring probabilistic models. The screenshot shows the frontend of Lumen.

![Lumen Frontend](/images/leapp.png) 